{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pylab as plt\n",
    "import corner\n",
    "import numdifftools as nd\n",
    "import glob\n",
    "import scipy.optimize as so\n",
    "import scipy.linalg as sl\n",
    "from PTMCMCSampler import PTMCMCSampler\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class GaussianLikelihood(object):\n",
    "    \n",
    "    def __init__(self, ndim=2, pmin=-10, pmax=10):\n",
    "        \n",
    "        self.a = np.ones(ndim)*pmin\n",
    "        self.b = np.ones(ndim)*pmax\n",
    "        \n",
    "    def lnlikefn(self, x):\n",
    "        return -0.5*np.sum(x**2)-len(x)*0.5*np.log(2*np.pi)\n",
    "    \n",
    "    def lnlikefn_grad(self, x):\n",
    "        ll = -0.5*np.sum(x**2)-len(x)*0.5*np.log(2*np.pi)\n",
    "        ll_grad = -x\n",
    "        return ll, ll_grad\n",
    "    \n",
    "    def lnpriorfn(self, x):\n",
    "        \n",
    "        if np.all(self.a <= x) and np.all(self.b >= x):\n",
    "            return 0.0\n",
    "        else:\n",
    "            return -np.inf  \n",
    "        return 0.0\n",
    "    \n",
    "    def lnpriorfn_grad(self, x):\n",
    "        return self.lnpriorfn(x), np.zeros_like(x)\n",
    "    \n",
    "    def lnpost_grad(self, x):\n",
    "        ll, ll_grad = self.lnlikefn_grad(x)\n",
    "        lp, lp_grad = self.lnpriorfn_grad(x)\n",
    "        return ll+lp, ll_grad+lp_grad\n",
    "    \n",
    "    def lnpost(self, x):\n",
    "        return lnpost_grad(x)[0]\n",
    "    \n",
    "    def hessian(self, x):\n",
    "        return -np.eye(len(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "class intervalTransform(object):\n",
    "    \"\"\"\n",
    "    Wrapper class of the likelihood for Hamiltonian samplers. This implements a\n",
    "    coordinate transformation for all parameters from an interval to all real numbers.\n",
    "    \"\"\"\n",
    "    def __init__(self, likob, pmin=None, pmax=None):\n",
    "        \"\"\"Initialize the intervalLikelihood with a ptaLikelihood object\"\"\"\n",
    "        self.likob = likob\n",
    "        \n",
    "        if pmin is None:\n",
    "            self.a = likob.a\n",
    "        else:\n",
    "            self.a = pmin * np.ones_like(likob.a)\n",
    "        \n",
    "        if pmax is None:\n",
    "            self.b = likob.b\n",
    "        else:\n",
    "            self.b = pmax * np.ones_like(likob.b)\n",
    "\n",
    "    def forward(self, x):\n",
    "        \"\"\"Forward transform the real coordinates (on the interval) to the\n",
    "        transformed coordinates (on all real numbers)\n",
    "        \"\"\"\n",
    "        p = np.atleast_2d(x.copy())\n",
    "        posinf, neginf = (self.a == x), (self.b == x)\n",
    "        m = ~(posinf | neginf)\n",
    "        p[:,m] = np.log((p[:,m] - self.a[m]) / (self.b[m] - p[:,m]))\n",
    "        p[:,posinf] = np.inf\n",
    "        p[:,neginf] = -np.inf\n",
    "        return p.reshape(x.shape)\n",
    "\n",
    "    def backward(self, p):\n",
    "        \"\"\"Backward transform the transformed coordinates (on all real numbers)\n",
    "        to the real coordinates (on the interval)\n",
    "        \"\"\"\n",
    "        x = np.atleast_2d(p.copy())\n",
    "        x[:,:] = (self.b[:] - self.a[:]) * np.exp(x[:,:]) / (1 +\n",
    "                np.exp(x[:,:])) + self.a[:]\n",
    "        return x.reshape(p.shape)\n",
    "    \n",
    "    def logjacobian_grad(self, p):\n",
    "        \"\"\"Return the log of the Jacobian at point p\"\"\"\n",
    "        lj = np.sum( np.log(self.b[:]-self.a[:]) + p[:] -\n",
    "                2*np.log(1.0+np.exp(p[:])) )\n",
    "\n",
    "        lj_grad = np.zeros_like(p)\n",
    "        lj_grad[:] = (1 - np.exp(p[:])) / (1 + np.exp(p[:]))\n",
    "        return lj, lj_grad\n",
    "\n",
    "    def dxdp(self, p):\n",
    "        \"\"\"Derivative of x wrt p (jacobian for chain-rule) - diagonal\"\"\"\n",
    "        pp = np.atleast_2d(p)\n",
    "        d = np.ones_like(pp)\n",
    "        d[:,:] = (self.b[:]-self.a[:])*np.exp(pp[:,:])/(1+np.exp(pp[:,:]))**2\n",
    "        return d.reshape(p.shape)\n",
    "\n",
    "    def d2xd2p(self, p):\n",
    "        \"\"\"Derivative of x wrt p (jacobian for chain-rule) - diagonal\"\"\"\n",
    "        pp = np.atleast_2d(p)\n",
    "        d = np.zeros_like(pp)\n",
    "        d[:,:] = (self.b[:]-self.a[:])*(np.exp(2*pp[:,:])-np.exp(pp[:,:]))/(1+np.exp(pp[:,:]))**3\n",
    "        return d.reshape(p.shape)\n",
    "\n",
    "    def logjac_hessian(self, p):\n",
    "        \"\"\"The Hessian of the log-jacobian\"\"\"\n",
    "        # p should not be more than one-dimensional\n",
    "        assert len(p.shape) == 1\n",
    "\n",
    "        return np.diag(-2*np.exp(p) / (1+np.exp(p))**2)\n",
    "\n",
    "    def lnlikefn_grad(self, p, **kwargs):\n",
    "        \"\"\"The log-likelihood in the new coordinates\"\"\"\n",
    "        x = self.backward(p)\n",
    "        ll, ll_grad = self.likob.lnlikefn_grad(x, **kwargs)\n",
    "        lj, lj_grad = self.logjacobian_grad(p)\n",
    "        return ll+lj, ll_grad*self.dxdp(p)+lj_grad\n",
    "    \n",
    "    def lnlikefn(self, p, **kwargs):\n",
    "        return self.lnlikefn_grad(p)[0]\n",
    "\n",
    "    def lnpriorfn_grad(self, p, **kwargs):\n",
    "        \"\"\"The log-prior in the new coordinates. Do not include the Jacobian\"\"\"\n",
    "        x = self.backward(p)\n",
    "        lp, lp_grad = self.likob.lnpriorfn_grad(x)\n",
    "        return lp, lp_grad*self.dxdp(p)\n",
    "    \n",
    "    def lnpriorfn(self, p, **kwargs):\n",
    "        return self.lnpriorfn_grad(p)[0]\n",
    "\n",
    "    def logpostfn_grad(self, p, **kwargs):\n",
    "        \"\"\"The log-posterior in the new coordinates\"\"\"\n",
    "        x = self.backward(p)\n",
    "        lp, lp_grad = self.likob.lnpost_grad(x)\n",
    "        lj, lj_grad = self.logjacobian_grad(p)\n",
    "        return lp+lj, lp_grad*self.dxdp(p)+lj_grad\n",
    "\n",
    "\n",
    "    def hessian(self, p):\n",
    "        \"\"\"The Hessian matrix in the new coordinates\"\"\"\n",
    "        # p should not be more than one-dimensional\n",
    "        assert len(p.shape) == 1\n",
    "\n",
    "        # Get quantities from un-transformed distribution\n",
    "        x = self.backward(p)\n",
    "        orig_hessian = self.likob.hessian(x)\n",
    "        _, orig_lp_grad = self.likob.lnpost_grad(x)\n",
    "\n",
    "        # Transformation properties\n",
    "        hessian = self.logjac_hessian(p)\n",
    "        dxdpf = np.diag(self.dxdp(p))\n",
    "\n",
    "        hessian += np.dot(dxdpf.T, np.dot(orig_hessian, dxdpf))\n",
    "        hessian -= np.diag(self.d2xd2p(p)*orig_lp_grad)\n",
    "\n",
    "        return hessian\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "ndim = 50\n",
    "glo = GaussianLikelihood(ndim=ndim, pmin=0.0, pmax=10.0)\n",
    "glt = intervalTransform(glo, pmin=0.0, pmax=10)\n",
    "gl = glt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Demonstrate that the gradients are accurate\n",
    "p0 = np.ones(ndim)*0.01\n",
    "ndjac = nd.Jacobian(gl.lnlikefn)\n",
    "ndhes = nd.Hessian(gl.lnlikefn)\n",
    "ndhesd = nd.Hessdiag(gl.lnlikefn)\n",
    "\n",
    "print p0[:4]\n",
    "print gl.lnlikefn_grad(p0)[1][:4]\n",
    "print ndjac(p0)[:4]\n",
    "print np.diag(gl.hessian(p0))[:4]\n",
    "print ndhesd(p0)[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Maximize using scipy\n",
    "result = so.minimize(lambda x: -gl.lnlikefn(x), p0, jac=lambda x: -gl.lnlikefn_grad(x)[1],\n",
    "                     method='Newton-CG', hess=lambda x: -gl.hessian(x), options={'disp':True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Set the start position and the covariance\n",
    "p0 = result['x']\n",
    "h0 = gl.hessian(p0)\n",
    "cov = sl.cho_solve(sl.cho_factor(-h0), np.eye(len(h0)))\n",
    "print np.diag(cov)[:10]\n",
    "cov = np.diag(np.ones(ndim)*0.1)\n",
    "#p0 = np.ones(ndim)*0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler = PTMCMCSampler.PTSampler(ndim, gl.lnlikefn, gl.lnpriorfn, cov,\n",
    "                                  logl_grad=gl.lnlikefn_grad, logp_grad=gl.lnpriorfn_grad,\n",
    "                                  outDir='./chains')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "sampler.sample(p0, 40000, burn=10000, thin=1,\n",
    "               SCAMweight=10, AMweight=10, DEweight=10, NUTSweight=10, HMCweight=10,\n",
    "               HMCsteps=50, HMCstepsize=0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "jumpfiles = glob.glob('chains/*jump.txt')\n",
    "jumps = map(np.loadtxt, jumpfiles)\n",
    "for ct, j in enumerate(jumps):\n",
    "    plt.plot(j, label=jumpfiles[ct].split('/')[-1].split('_jump.txt')[0])\n",
    "plt.legend(loc='best', frameon=False)\n",
    "plt.ylabel('Acceptance Rate')\n",
    "plt.ylim(0.0, 1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "data = np.loadtxt('chains/chain_1.txt')\n",
    "chaint = data[:,:-4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "plt.plot(data[1000:,-4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "chain = glt.backward(chaint)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corner.corner(chaint[:,:3], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "corner.corner(chain[:,:3], bins=50);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
